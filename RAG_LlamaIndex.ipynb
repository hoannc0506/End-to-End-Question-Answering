{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Machine Learing questions using Gemma and LlamaIndex\n",
    "- LLMs can be fine-tuned to achieve several common tasks but it still cannot being aware of specific content.\n",
    "- **RAG (Retrieval Augmented Generation)** is a popular approach to address such knowledge-intensive tasks (legal, medical, technical, ...)\n",
    "- This notebook demonstrates how you can quickly build a RAG to answer **machine learning questions** with additional knowledge in a machine learning book.\n",
    "- Let's see how Gemma answer questions with and without additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "- RAG takes an input and retrieves a set of relevant documents given a source\n",
    "- The process to build a RAG with LlamaIndex generally consists of two stages:\n",
    "    + **Idexing Stage:** LlamaIndex prepares the knowledge base by ingesting data and converting it into Documents\n",
    "    + **Querying stage:** Relevant context is retrieved from the knowledge base to assist the model in responding to queries.\n",
    " \n",
    "![RAG pipeline with LlamaIndex](https://global.discourse-cdn.com/business7/uploads/streamlit/original/3X/a/e/ae647f8c23a1b60fbcf59fd7c4f0a33aee9ce255.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexing Stage\n",
    "In this stage, we collect data from a machine learning book and store it into a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data Collection\n",
    "First, we extract infomation from a book [Python for Data Analysis](https://wesmckinney.com/book/) and store as **html files** in folder \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://wesmckinney.com/book/\"\n",
    "\n",
    "# Function to extract pages url \n",
    "def extract_pages_url():\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    chapter_links = []\n",
    "    \n",
    "    for li_tag in soup.find_all('li', class_=\"sidebar-item\"):\n",
    "        link = li_tag.find('a')\n",
    "        try:\n",
    "            chapter_links.append(\"https://wesmckinney.com\" + link['href'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return chapter_links[2:] # skip the first 2 section, as they are just introduction\n",
    "\n",
    "def extract_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove script and style tags\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "\n",
    "    # Get rid of empty tags\n",
    "    for tag in soup.find_all():\n",
    "        if not tag.text.strip():\n",
    "            tag.extract()\n",
    "    \n",
    "    content = soup.find(\"main\", class_=\"content\")\n",
    "    return content.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://wesmckinney.com/book/preliminaries',\n",
       " 'https://wesmckinney.com/book/python-basics',\n",
       " 'https://wesmckinney.com/book/python-builtin',\n",
       " 'https://wesmckinney.com/book/numpy-basics',\n",
       " 'https://wesmckinney.com/book/pandas-basics',\n",
       " 'https://wesmckinney.com/book/accessing-data',\n",
       " 'https://wesmckinney.com/book/data-cleaning',\n",
       " 'https://wesmckinney.com/book/data-wrangling',\n",
       " 'https://wesmckinney.com/book/plotting-and-visualization',\n",
       " 'https://wesmckinney.com/book/data-aggregation',\n",
       " 'https://wesmckinney.com/book/time-series',\n",
       " 'https://wesmckinney.com/book/modeling',\n",
       " 'https://wesmckinney.com/book/data-analysis-examples',\n",
       " 'https://wesmckinney.com/book/advanced-numpy',\n",
       " 'https://wesmckinney.com/book/ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_url = extract_pages_url()\n",
    "pages_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:13<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for page_url in tqdm(pages_url, desc=\"Collecting data\"):\n",
    "    section_name = page_url.split(\"/\")[-1]\n",
    "    cleaned_html = extract_page(page_url)\n",
    "\n",
    "    # Write cleaned HTML content to a new file\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    with open(f'./data/wesmckinney_{section_name}.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wesmckinney_accessing-data.html\n",
      "wesmckinney_advanced-numpy.html\n",
      "wesmckinney_data-aggregation.html\n",
      "wesmckinney_data-analysis-examples.html\n",
      "wesmckinney_data-cleaning.html\n",
      "wesmckinney_data-wrangling.html\n",
      "wesmckinney_ipython.html\n",
      "wesmckinney_modeling.html\n",
      "wesmckinney_numpy-basics.html\n",
      "wesmckinney_pandas-basics.html\n",
      "wesmckinney_plotting-and-visualization.html\n",
      "wesmckinney_preliminaries.html\n",
      "wesmckinney_python-basics.html\n",
      "wesmckinney_python-builtin.html\n",
      "wesmckinney_time-series.html\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Ingestion \n",
    "In this step, we create a vector database with LlamaIndex. \n",
    "- First, we load documents from book chapters we have extract bellow\n",
    "- Second, we use an Embedding model to vectors embedding\n",
    "- Third, these documents embeddings will store in a database, in this practice, we use Chromadb - a local database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.readers.file import HTMLTagReader, PDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read documents from folder data\n",
    "documents = SimpleDirectoryReader(\"./data\", \n",
    "                                  file_extractor={\".pdf\": PDFReader(), \n",
    "                                                  \".html\": HTMLTagReader(tag='main', ignore_no_id=True)\n",
    "                                                 }\n",
    "                                 ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56923, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0].text), len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document is a chapter in our reference book, so it quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag': 'main',\n",
       " 'tag_id': 'quarto-document-content',\n",
       " 'file_path': '/workspace/GemmaRAG/data/wesmckinney_accessing-data.html',\n",
       " 'file_name': 'wesmckinney_accessing-data.html',\n",
       " 'file_type': 'text/html',\n",
       " 'file_size': 143688,\n",
       " 'creation_date': '2024-03-12',\n",
       " 'last_modified_date': '2024-03-12'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "     \n",
      "\n",
      "      Data Loading, Storage, and File Formats\n",
      "This Open Access web version of\n",
      "     \n",
      "      Python for Data Analysis 3rd Edition\n",
      "     \n",
      "     is now available as a companion to the\n",
      "     \n",
      "      print and digital editions\n",
      "     \n",
      "     . If you encounter any errata,\n",
      "     \n",
      "      please report them here\n",
      "     \n",
      "     . Please note that some aspects of this site as produced by Quarto will differ from the formatting of the print and eBook versions from O’Reilly.\n",
      "    \n",
      "\n",
      "     If you find the online edition of the book useful, please consider\n",
      "     \n",
      "      ordering a paper copy\n",
      "     \n",
      "     or a\n",
      "     \n",
      "      DRM-free eBook\n",
      "     \n",
      "     to support the author. The content from this website may not be copied or reproduced. The code examples are MIT licensed and can be found on GitHub or Gitee.\n",
      "Reading data and making it accessible (often called\n",
      "  \n",
      "   data loading\n",
      "  \n",
      "  ) is a necessary first step for using most of the tools in this book. The term\n",
      "  \n",
      "   parsing\n",
      "  \n",
      "  is also sometimes used to describe loading\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InstructorEmbedding(model_name='intfloat/multilingual-e5-large', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fa653a38dc0>, query_instruction=None, text_instruction=None, cache_folder='./models')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load document embedding model\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = InstructorEmbedding(model_name=\"intfloat/multilingual-e5-large\", cache_folder=\"./models\", device=device_type)\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Collection(name=chroma_db), 1023)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init chroma db\n",
    "# Creates a persistent instance of Chroma that saves to disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"./DB\")\n",
    "# Get or create a collection with the given name and metadata.\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"chroma_db\")\n",
    "chroma_collection, chroma_collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are stored within a ChromaDB collection.\n",
    "During query time, the index uses ChromaDB to query for the top k most similar nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abb839741024c839e96e4610c0f5718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cd8df5d2dc43d582cec765feab12f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create index from documents.\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Querying Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Huggingface access token ········\n"
     ]
    }
   ],
   "source": [
    "# to load gemma model, we need to get huggingface access token and request access to gemma\n",
    "HF_ACCESS_TOKEN = getpass('Huggingface access token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d841a72e19874c3cbb6be63c80ac804f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                          token=HF_ACCESS_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", \n",
    "                                             device_map=device,\n",
    "                                             # torch_dtype=torch.float16,\n",
    "                                             token=HF_ACCESS_TOKEN\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=8192,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=device,\n",
    "    system_prompt=system_prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_hf = index.as_query_engine(llm=llm_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 24.5 ms, total: 2.23 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response_hf = query_engine_hf.query(\"what is linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression is a statistical method that models a relationship between a continuous dependent variable and one or more independent variables.\n"
     ]
    }
   ],
   "source": [
    "print(response_hf.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8c7abda3-9ea9-4567-8a60-e3e61e3af804': {'tag': 'main',\n",
       "  'tag_id': 'quarto-document-content',\n",
       "  'file_path': '/workspace/GemmaRAG/data/wesmckinney_modeling.html',\n",
       "  'file_name': 'wesmckinney_modeling.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 90881,\n",
       "  'creation_date': '2024-03-10',\n",
       "  'last_modified_date': '2024-03-10'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_hf.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how gemma work without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<start_of_turn>user\n",
      "You are a Q&A assistant. Your goal is to answer questions as accurately as possible\n",
      "Question: what is linear regression?\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      " \n"
     ]
    }
   ],
   "source": [
    "question = \"what is linear regression?\"\n",
    "prompt = f\"\"\"\n",
    "<start_of_turn>user\n",
    "You are a Q&A assistant. Your goal is to answer questions as accurately as possible\n",
    "Question: {question}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    " \"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 8.82 ms, total: 2.11 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "respones_without_rag = text_generation_pipeline(prompt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is a method of data analysis that is used to find a relationship between two variables. It is a regression analysis that uses a least squares approach to find a linear relationship between two variables. It is also a method of visualization that uses a scatter plot to display a linear relationship between two variables.\n"
     ]
    }
   ],
   "source": [
    "print(respones_without_rag['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Python for Data Analysis](https://wesmckinney.com/book/)\n",
    "2. [Prompt engineer guide - RAG](https://www.promptingguide.ai/techniques/rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
